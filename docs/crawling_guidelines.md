# 웹소설 데이터 수집 가이드라인

> **웹소설나침반 프로젝트** - AI 기반 개인화 웹소설 추천 시스템

## 📋 개요

본 가이드라인은 웹소설 추천 시스템 개발을 위한 데이터 수집 과정에서 준수해야 할 법적, 기술적, 윤리적 기준을 제시합니다.

## ⚖️ 법적 준수사항

### 1. **robots.txt 엄격 준수**

#### 카카오페이지 크롤링 정책
```
# 허용된 크롤링 범위
✅ /content/{id}                    # 작품 상세 페이지
✅ /content/{id}?tab_type=about     # 작품 정보 탭
✅ /menu/10011                      # 웹소설 카테고리
✅ /landing/ranking/                # 랭킹 페이지
✅ /search/                         # 검색 결과

# 금지된 크롤링 범위
❌ /viewer                          # 뷰어 페이지 (절대 금지)
❌ /store/kakaopage/webseries/viewer # 웹소설 뷰어 (절대 금지)
```

### 2. **수집 가능/금지 데이터**

#### ✅ 수집 가능한 데이터
- **메타데이터**: 제목, 작가명, 장르, 연재상태, 연령등급
- **공개 통계**: 조회수, 평점, 댓글수, 랭킹 순위
- **분류 정보**: 키워드, 태그, 카테고리
- **공개 소개**: 시놉시스, 작품 소개
- **커버 이미지**: 썸네일 이미지 URL

#### ❌ 수집 금지 데이터
- **본문 콘텐츠**: 웹소설 실제 내용 (저작권 보호)
- **유료 콘텐츠**: 결제가 필요한 모든 콘텐츠
- **개인정보**: 사용자 개인 데이터
- **내부 API**: 비공개 API 엔드포인트

## 🛠️ 기술적 구현 원칙

### 1. **크롤링 기본 원칙**

```python
# robots.txt 준수 확인 필수
import urllib.robotparser

def check_robots_txt(url):
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url("https://page.kakao.com/robots.txt")
    rp.read()
    return rp.can_fetch("*", url)
```

### 2. **요청 제한 및 딜레이**

- **최소 딜레이**: 2-5초 (사람과 유사한 패턴)
- **최대 요청**: 시간당 500회 이하
- **User-Agent**: 실제 브라우저 에이전트 사용
- **세션 관리**: 쿠키 및 세션 유지

### 3. **에러 처리**

```python
# 기본 에러 처리 패턴
try:
    # robots.txt 확인
    if not check_robots_txt(url):
        raise ValueError("robots.txt 위반")
    
    # 딜레이 적용
    time.sleep(random.uniform(2, 5))
    
    # 요청 실행
    response = requests.get(url, headers=headers, timeout=10)
    
except Exception as e:
    logging.error(f"크롤링 실패: {url}, 오류: {e}")
```

## 📊 데이터 수집 범위

### 1. **우선순위별 수집 대상**

#### Phase 1: 핵심 메타데이터
- 작품 기본 정보 (제목, 작가, 장르)
- 연재 상태 및 회차 정보
- 평점 및 조회수

#### Phase 2: 상세 정보
- 시놉시스 및 키워드
- 랭킹 및 카테고리 정보
- 커버 이미지

#### Phase 3: 확장 데이터
- 작가별 작품 목록
- 시리즈 관계 정보
- 이벤트 및 프로모션 정보

### 2. **데이터 품질 기준**

```python
# 필수 검증 항목
required_fields = [
    'content_id', 'title', 'author', 
    'main_genre', 'status', 'view_count'
]

# 데이터 검증 함수
def validate_metadata(data):
    for field in required_fields:
        if field not in data or not data[field]:
            raise ValueError(f"필수 필드 누락: {field}")
```

## 🔄 크롤링 운영 정책

### 1. **수집 주기**

- **신작 수집**: 매일 1회 (오전 2시)
- **랭킹 업데이트**: 매일 2회 (오전 6시, 오후 6시)
- **전체 메타데이터**: 주 1회 (일요일 새벽)
- **실패 URL 재시도**: 매일 1회 (오전 4시)

### 2. **모니터링 지표**

- **성공률**: 95% 이상 유지
- **응답 시간**: 평균 3초 이하
- **에러율**: 5% 이하
- **일일 수집량**: 목표치 대비 달성률

### 3. **장애 대응**

- **IP 차단 시**: 24시간 대기 후 재시도
- **높은 에러율**: 자동 크롤링 중단 및 알림
- **서버 응답 없음**: 30분 간격으로 3회 재시도

## 📋 개발 체크리스트

### 크롤링 코드 개발 시 필수 확인사항

- [ ] robots.txt 준수 여부 확인
- [ ] 적절한 딜레이 적용 (2-5초)
- [ ] 실제 브라우저 헤더 사용
- [ ] /viewer 경로 접근 금지 확인
- [ ] 에러 처리 및 로깅 구현
- [ ] 중복 요청 방지 로직
- [ ] 데이터 검증 로직 포함

### 배포 전 필수 검토사항

- [ ] 법적 검토 완료
- [ ] 성능 테스트 완료
- [ ] 모니터링 시스템 구축
- [ ] 장애 대응 절차 수립
- [ ] 데이터 품질 검증 완료

## 🚨 주의사항

### 절대 금지 행위
1. **뷰어 페이지 크롤링**: `/viewer` 경로 절대 접근 금지
2. **대량 동시 요청**: 서버 부하 유발하는 과도한 요청
3. **개인정보 수집**: 사용자 개인 데이터 수집 금지
4. **robots.txt 무시**: 명시적 금지 사항 위반

### 권장 사항
1. **점진적 확장**: 소규모부터 시작하여 단계적 확장
2. **모니터링 강화**: 실시간 상태 확인 및 알림 시스템
3. **백업 계획**: 수집 실패 시 대체 방안 준비
4. **정기 검토**: 월 1회 가이드라인 준수 여부 점검

---

**최종 업데이트**: 2024년 12월
**담당자**: 웹소설나침반 개발팀
**검토**: 법무팀, 기술팀